\documentclass[10pt]{article}
\usepackage{amsmath, mathtools, amsfonts, bm, amssymb, amsthm,graphicx,epstopdf, caption}%, subfigure}
\usepackage [top = 1.5cm, left = 1.5cm, right = 1.5cm, bottom = 1.5cm]{geometry}%, left = 1.5cm, right = 1.8cm, bottom = 1.5cm]{geometry}
\usepackage{algorithm, algorithmic}
\usepackage{appendix}
\usepackage{bm}
\title{\textbf{Noisy Power method}}
\author{}
\date{}
\usepackage{tikz}
\newcommand{\yt}{\bm{y}_t}
\newcommand{\by}{\bm{y}}
\newcommand{\lt}{\bm{\ell}_t}
\newcommand{\wt}{\bm{w}_t}
\newcommand{\pt}{\bm{U}}
\newcommand{\at}{\bm{a}_t}
\newcommand{\mt}{\bm{M}_t}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bq}{\bm{q}}
\newcommand{\bs}{\bm{s}}
\newcommand{\K}{\bm{K}}

\usepackage{amsmath,amssymb,amsthm, bm}
\newtheorem{theorem}{Theorem}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{sigmodel}[theorem]{Assumption}%{Model}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{ass}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{heuristic}[theorem]{Heuristic}
\newcommand{\norm}[1]{\left\|#1\right\|}
 \newcommand{\indep}{\perp \!\!\!\perp}

%\renewcommand{\subsubsection}[1]{{\bf #1. }}
\renewcommand{\qedsymbol}{$\boxtimes$}

\newcommand{\nsrmax}{\text{NSR}}
\newcommand{\nsrmin}{\text{NSR\_min}}

\newcommand{\thresh}{\mathrm{thresh}}

\newcommand{\ind}{\mathrm{ind}}
\newcommand{\SE}{\mathrm{SE}}

\usepackage{color}
\begin{document}
\maketitle


\section{Problem Statement}
In this document we study the problem of analyzing the ``noisy power method''. We are given a fixed matrix $A \in \R^{n \times n}$ such that $A = A^T$ and $A \succeq 0$. Furthermore, we assume that there exist orthonormal eigenvectors, $u_1^*, \cdots u_n^*$ such that $A u_i^* = \lambda_i u_i^*$ for all $i$ and without loss of generality, we set $\lambda_1 > \lambda_2 \geq \lambda_3 \geq \cdots \geq \lambda_n \geq 0$. In other words, we have $A = U^* \Lambda (U^*)^T$ The goal is to iteratively compute an estimate of the top eigenvector, $u_1^*$. 

However, due to various reasons, we cannot directly use the power method, and instead we can only use an approximation since we are able to only access noisy matrix-vector/matrix-matrix  products. We will use the following algorithm

\section{Algorithm}
In this document we assume that at each iteration, $t$, we only access the noisy versions of the matrix vector/matrix matrix products, i.e., we analyze the following algorithm

\newcommand{\Y}{\bm{Y}}
\newcommand{\z}{\bm{z}}
\begin{algorithm}[H]
\caption{Noisy power method -- rank 1}\label{algo:rank1}
  \begin{algorithmic}[1]
    \REQUIRE $A$ 
    \STATE $u_0 \sim \mathcal{N}(0, I_{n\times n})$ (Initialization) 
    \STATE $u_0 = u_0/\|u_0\|_2$ (step is excluded currently in analysis)
    \FOR{$t=1,\dots, T$}
    \STATE $w_t \sim \mathcal{N}(0, \sigma_c^2 I_{n\times n})$
    \STATE $u_t = A u_{t-1} + w_t$ 
    \STATE $u_t = u_t / \|u_t\|_2$ (step is excluded currently in analysis)
    \ENDFOR
    \ENSURE $u_T$ 
  \end{algorithmic}
\end{algorithm}

\newcommand{\E}{\mathbb{E}}
\section{High probability bounds on $\sin\theta$ for rank $1$ setting}

We assume that the channel noise, $w_t$ at each iteration is identically distributed, and is independent of $A$ (if this is random), $u_0$ and all the previous $w_{1:t-1}$. Notice that we can write the iterates as follows
\begin{gather*}
u_0 = u_0 \\
u_1 = A u_0 + w_1 \\
u_2 = A u_1 + w_2 = A^2 u_0 + A w_1 + w_2\\
\vdots \\
u_t = A^t u_0 + \sum_{\tau = 1}^t A^{t-\tau} w_\tau
\end{gather*}
For the purpose of analysis, we write $u_0 = \alpha_1 u_1^* + \alpha_2 u_2^* + \cdots + \alpha_n u_n^*$ and notice that $\sum_i \alpha_i^2 = \|u_0\|_2^2$. Similarly, for all $t$, we write $w_t = \beta_{t,a} u_1^* + \beta_{t,2} u_2^* +  \cdots + \beta_{t,n}^* u_n^*$, and here too, $\sum_i \beta_{t,i}^2 = \|w_t\|_2^2$. 

Observe that the $\alpha_i$'s and $\beta_{t,i}$'s are zero-mean random variables, and $\E[\alpha_i^2] = 1$ and $\E[\beta_{t,i}^2] = \sigma_c^2$

We can write the expression for $u_t$ as follows
\begin{align*}
u_t &= A^t u_0 + \sum_{\tau=1}^t A^{t - \tau} w_\tau \\
&= A^t \left( \sum_{i=1}^n \alpha_i u_i^* \right) + \sum_{\tau=1}^t A^{t - \tau} \left( \sum_{i=1}^n \beta_{\tau,i} u_i^* \right) \\
&= \sum_{i=1}^n \alpha_i (A^t u_i^*)  + \sum_{\tau=1}^t \sum_{i=1}^n \beta_{\tau,i} (A^{t - \tau} u_i^*)  \\
&= \sum_{i=1}^n \alpha_i (\lambda_i^t u_i^*)  + \sum_{\tau=1}^t \sum_{i=1}^n \beta_{\tau,i} (\lambda_i^{t - \tau} u_i^*)  \\
&= \sum_{i=1}^n \alpha_i \lambda_i^t u_i^*  + \sum_{i=1}^n \sum_{\tau=1}^t  \beta_{\tau,i} \lambda_i^{t - \tau} u_i^*  \\
&= \sum_{i=1}^n \left(\alpha_i \lambda_i^t + \sum_{\tau=1}^t  \beta_{\tau,i} \lambda_i^{t - \tau}\right) u_i^*  \\
&:= \sum_{i=1}^n \delta_{t,i}  u_i^*
\end{align*}

%\section{}


Now, we will bound the sine of the angle between the estimate at iteration $t$ and the desired eigenvector, $u_1^*$. The expression is given as 
\begin{align*}
\sin^2\theta(u_1^*, u_t) &= \norm{u_1^* - \frac{u_t}{\|u_t\|^2} \langle u_t, u_1^*\rangle}^2 = \norm{u_1^* - \frac{u_t}{\|u_t\|^2} \delta_{t,1}}^2 \\
&= (u_1^*)^Tu_1^* + \frac{u_t^T u_t (\delta_{t,1}^2)}{\|u_t\|^4} - 2 \frac{(u_1^*)^T u_t (\delta_{t,1})}{\|u_t\|^2 } \\
&= 1 - \frac{\delta_{t,1}^2}{\|u_t\|^2} = 1 - \cos^2\theta(u_1^*, u_t)
\end{align*}

%{\color{red} change some of this to include tail bounds for Gaussian r.v. with proper constants }

we will now derive high probability bounds on $\delta_{t,1}$, $\|u_t\|$. Firstly, notice that for any $t,i$, $\delta_{t,i}$ is a Gaussian r.v. since it is the linear combination of $t+1$ gaussian r.v.'s. Furthermore, $\E[\delta_{t,i}] = 0$ and 
\begin{align*}
\E[\delta_{t,i}^2] = \E[(\alpha_i \lambda_i^t + \sum_{\tau=1}^t \beta_{\tau,i} \lambda_i^{t-\tau})^2] = \lambda_i^{2t} + \sigma_c^2 \sum_{\tau=1}^t \lambda_i^{2(t-\tau)} = \lambda_i^{2t} + \sigma_c^2 \frac{\lambda_i^{2t}-1}{\lambda_i^2-1} := \sigma_{t,i}^2
\end{align*}
{\color{blue} \noindent ??current writing can be simplified by using $\sin \theta \leq \tan \theta $ -- mostly done}
%\noindent For purpose of analysis, we will consider the case of $i=1$ and $i > 1$ separately. In particular, since $\delta_{t,1}/\sigma_{t,1} \sim \mathcal{N}(0,1)$, from standard Gaussian tail bound it follows that
%\begin{align*}
%\Pr(
%\end{align*}

%\subsection{$2$ non-zero eigenvalues}
%For simplicity, consider the case when only eigenvalues are non-zero, i.e., $\delta_{t,i} = 0$, $i \geq 3$. {\color{red} (this is not completely correct - the $\delta_{t,i} = \beta_{t,i}$ for $i > 2$. Thus, we will again need to use a sum of exponential tail bound as done in the general case)}
%
For any given $t,i$, since $\delta_{t,i}$ is Gaussian, we know that $Y_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$ is a zero-mean sub exponential r.v. with $\|Y_{t,i}\|_{\psi_1} \leq 2 \sigma_{t,i}^2$. Thus, using the tail bound for sub-exponential r.v.'s we know that
%
%\begin{align}\label{eq:del_bnd}
%\Pr\left( Y_{t,i} \geq \epsilon_{t,i} \right) \leq \exp\left[-c\min\left(\frac{\epsilon_{t,i}^2}{\sigma_{t,i}^4}, \frac{\epsilon_{t,i}}{\sigma_{t,i}^2} \right)\right]
%\end{align}
%
%
%Now, for some $\gamma \in (0,1)$, set $\epsilon_{t,1} = \gamma_0 \sigma_{t,1}^2$ and $\epsilon_{t,2} = \gamma_1 \sigma_{t,2}^2$.
%Thus,
%\begin{align*}
%&\Pr\left( \delta_{t,1}^2 \geq (1 - \gamma_0) \sigma_{t,1}^2 \right) \geq 1 - \exp(-c \gamma_0^2) \\ 
%&\Pr\left( \delta_{t,2}^2 \leq (1 + \gamma_1) \sigma_{t,2}^2 \right) \geq 1 - \exp(-c \gamma_1^2) \\ 
%\end{align*}
%
%And thus, with probability at least $ 1 - \exp(-c \gamma_0^2) - \exp(-c \gamma_1^2)$ %(also assume that $\lambda_1 > \lambda_2 \geq 1$),
%\begin{align*}
%\frac{\delta_{t,2}^2}{\delta_{t,1}^2} &\leq  \left(\frac{1 + \gamma_1}{1 - \gamma_0}\right) \frac{\sigma_{t,2}^2}{\sigma_{t,1}^2} = C_\gamma \frac{\sigma_{t,2}^2}{\sigma_{t,1}^2}
%\end{align*}
%Notice that
%\begin{align*}
%\tan^2\theta(u_t, u_1^*) = \frac{\delta_{t,2}^2}{\delta_{t,1}^2}
%\end{align*}
%and since $\sin\theta \leq \tan \theta$, it suffices to show that $\tan^2\theta \leq \epsilon^2$.
%%Notice that 
%%\begin{align*}
%%\cos^2\theta(u_t, u_1^*)  = \frac{1}{1 + \frac{\delta_{t,2}^2}{\delta_{t,1}^2}}
%%\end{align*}
%%and to obtain $\sin^2\theta(u_1,u_1^*) \leq \epsilon^2$, we require
%%
%%\begin{align*}
%%&1 - \epsilon^2\leq \cos^2\theta(u_t, u_1^*)  = \frac{1}{1 + \frac{\delta_{t,2}^2}{\delta_{t,1}^2}} %\\
%%%&\implies \frac{\delta_{t,2}^2}{\delta_{t,1}^2} \leq \epsilon^2 \leq \frac{\epsilon^2}{1 - \epsilon^2}
%%\end{align*}
%%and in fact, $\frac{\delta_{t,2}^2}{\delta_{t,1}^2} \leq \epsilon^2$ suffices since $\epsilon^2 \leq \frac{\epsilon^2}{1 - \epsilon^2}$.  
%
%Now we derive the dependence on the number of iterations reqired. 
%
%\begin{align*}
%C_\gamma \frac{\sigma_{t,2}^2}{\sigma_{t,1}^2} &= C_\gamma \frac{\lambda_2^{2t} + \sigma_c^2 \frac{\lambda_2^{2t}-1}{\lambda_2^2-1}}{\lambda_1^{2t} + \sigma_c^2 \frac{\lambda_1^{2t}-1}{\lambda_1^2-1}} = C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_2^{-2t} (\lambda_2^{2t-2} + \lambda_2^{2t-4} + \cdots 1)}{1 + \sigma_c^2 \lambda_1^{-2t} (\lambda_1^{2t-2} + \lambda_1^{2t-4} + \cdots 1)} \\
%&\leq C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_2^{-2t} (\lambda_2^{2t-2} + \lambda_2^{2t-4} + \cdots 1)}{1 + \nsrmax^2} \\ 
%&\leq C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} [1 + \sigma_c^2 \lambda_2^{-2t} (\lambda_2^{2t-2} + \lambda_2^{2t-4} + \cdots 1)] %}{1 + \nsrmax^2}
%\end{align*}
%where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio. 
%\subsubsection{Eigenvalue $\lambda_2 \geq 1$}
%For the case of $\lambda_2 \geq 1$, we have 
%\begin{align*}
%C_\gamma \frac{\sigma_{t,2}^2}{\sigma_{t,1}^2} \leq C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} (1 + t \sigma_c^2 / \lambda_2^2) \leq C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} (1 + t \sigma_c^2)
%\end{align*}
%And thus, as long as 
%\begin{align*}
%t \geq C\frac{1}{\log(\lambda_1/\lambda_2)} \log\left(\frac{C_{\gamma}  \max(1, t\sigma_c^2)}{\epsilon}\right)
%\end{align*}
%
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon)$.
%
%\subsubsection{Eigenvalue $\lambda_2 <1$}
%When $\lambda_2 <1$, we have
%\begin{align*}
%C_\gamma \frac{\sigma_{t,2}^2}{\sigma_{t,1}^2}  &= C_{\gamma} \left( \frac{1}{\lambda_1} \right)^{2t} \frac{\lambda_2^{2t} + \sigma_c^2 (\lambda_2^{2t-2} + \lambda_2^{2t-4} + \cdots 1)}{1 + \sigma_c^2 \lambda_1^{-2t} (\lambda_1^{2t-2} + \lambda_1^{2t-4} + \cdots 1)} \\
%&\leq C_{\gamma} \left( \frac{1}{\lambda_1} \right)^{2t} \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \leq  C_{\gamma} \left(\frac{1}{\lambda_1}\right)^{2t}(1 + t \sigma_c^2) \\
%\end{align*}
%and we notice that this decays slower than the case when $\lambda_2 \geq 1$. And thus, as long as 
%\begin{align*}
%t \geq C\frac{1}{\log \lambda_1} \log\left(\frac{C_{\gamma}  \max(1, t\sigma_c^2)}{\epsilon}\right)
%\end{align*}
%
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon)$.




%{\color{blue} 
%one possible way to get a uniform expression (as with the $\lambda_2 \geq 1$ case) is to define 
%\begin{align*}
%\nsrmin^2 = \frac{1 + t \sigma_c^2- \lambda_2^{2t}}{t \lambda_2^{2t}} \asymp \frac{\sigma_c^2}{\lambda_2^{2t}}
%\end{align*}
%but not sure if we should do this -- discuss
%}
%Thus, as long as $\sigma_c^2$ is small enough, the last term can be upper bounded by a small numerical constant. The first term can also be bounded above by a small constant by picking $\gamma_0,\gamma_1$ appropriately. The second term goes to $0$ as $t \to \infty$ and thus, we can ensure $\frac{\delta_{t,2}^2}{\delta_{t,1}^2} \leq \epsilon$ for any arbitrary $\epsilon$. 

%In the setting of $\lambda_2 \geq 1$, notice that irrespective of the value of $\sigma_c$, as long as 
%\begin{align*}
%t \geq C\frac{\log\left(\left(\frac{1 + \gamma_1}{1 - \gamma_0}\right)  \frac{\lambda_1^2}{\lambda_1^2 + \sigma_c^2}  \frac{\lambda_2^2 - 1 + \sigma_c^2}{\lambda_2^2-1} /\epsilon\right)}{\log(\lambda_1/\lambda_2)}
%\end{align*}
%with probablity at least $1 -  \exp(-c\gamma_0^2) -  \exp(-c\gamma_1^2) $, $\sin\theta(u_t, u_1^*) \leq \epsilon$. 


%And thus, as long as 
%\begin{align*}
%t \geq C\frac{\log\left(C_{\gamma}  \max(1, t\sigma_c^2) /\epsilon\right)}{\log(\lambda_1/\lambda_2)}
%\end{align*}
%
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon)$.







%\subsection{$n$ non-zero eigenvalues}
%The case when there are more than $2$ non-zero eigenvalues is very similar to the above case, with the exception that we will now use a different tail bound for the smaller singular values (use sum of sub-exponential as opposed to a single sub-exponential). 
%For simplicity, consider the case when only eigenvalues are non-zero, i.e., $\delta_{t,i} = 0$, $i \geq 3$. 

For any given $t,i$, since $\delta_{t,i}$ is Gaussian, we know that $Y_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$ is a zero-mean sub exponential r.v. with $\|Y_{t,i}\|_{\psi_1} \leq 2 \sigma_{t,i}^2$. Thus, using the tail bound for sub-exponential r.v.'s we know that
%As before, $Y_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$ is a zero-mean sub exponential r.v. with $\|Y_{t,i}\|_{\psi_1} \leq 2 \sigma_{t,i}^2$. 
Thus, using the tail bound for sum of sub-exponential r.v.'s we know that

\begin{align}\label{eq:del_bnd}
\Pr\left( \left| \sum_{i=2}^n Y_{t,i} \geq \epsilon_{t} \right| \right) \leq \exp\left[-c\min\left(\frac{\epsilon_{t}^2}{\sum_{i \geq 2} \sigma_{t,i}^4}, \frac{\epsilon_{t}}{\sigma_{t,2}^2} \right)\right]
\end{align}

%\begin{align*}
%\Pr\left( \bigg|\sum_i X_{t,i} \bigg| \geq \epsilon^* \sum_i \sigma_{t,i}^2 \right) \leq 2 \exp\left[ -c \min \left( \frac{(\epsilon^*)^2 (\sum_{i} \sigma_{t,i}^2)^2}{\sum \sigma_{t,i}^4}, \frac{\epsilon^* \sum_i \sigma_{t,i}^2}{\sigma_{t,1}^2} \right) \right]
%\end{align*}
setting $\epsilon_t = \gamma_1 \sum_{i \geq 2} \sigma_{t,2}^2$ for some $\gamma_1$, the first term dominates in the above probability expression so that we get a failure probability of $2 \exp(-c \gamma_1^2)$.


%Now, for some $\gamma \in (0,1)$, set $\epsilon_{t,1} = \gamma_0 \sigma_{t,1}^2$ and $\epsilon_{t,2} = \gamma_1 \sigma_{t,2}^2$.
Thus, 
\begin{align*}
&\Pr\left( \delta_{t,1}^2 \geq (1 - \gamma_0) \sigma_{t,1}^2 \right) \geq 1 - \exp(-c \gamma_0^2) \\ 
&\Pr\left( \sum_{i =2}^n \delta_{t,i}^2 \leq (1 + \gamma_1) \sum_{i=2}^n \sigma_{t,i}^2 \right) \geq 1 - \exp(-c \gamma_1^2) \\ 
\end{align*}

And thus, with probability at least $ 1 - \exp(-c \gamma_0^2) - \exp(-c \gamma_1^2)$ %(also assume that $\lambda_1 > \lambda_2 \geq 1$),
\begin{align*}
\sum_{i \geq 2}\frac{\delta_{t,i}^2}{\delta_{t,1}^2} &\leq  \left(\frac{1 + \gamma_1}{1 - \gamma_0}\right) \frac{\sum_{i\geq 2} \sigma_{t,2}^2}{\sigma_{t,1}^2} = C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2}
\end{align*}
Notice that
\begin{align*}
\tan^2\theta(u_t, u_1^*) = \frac{\sum_{i\geq 2}  \delta_{t,i}^2}{\delta_{t,1}^2}
\end{align*}
and since $\sin\theta \leq \tan \theta$, to prove the final result it suffices to show that $\tan^2\theta \leq \epsilon^2$.
%
%Notice that 
%\begin{align*}
%\cos^2\theta(u_t, u_1^*)  = \frac{1}{1 + \frac{\sum_{i\geq 2} \delta_{t,i}^2}{\delta_{t,1}^2}}
%\end{align*}
%and to obtain $\sin^2\theta(u_1,u_1^*) \leq \epsilon^2$, we require
%
%\begin{align*}
%&1 - \epsilon^2\leq \cos^2\theta(u_t, u_1^*)  = \frac{1}{1 + \frac{\sum_{i\geq 2} \delta_{t,i}^2}{\delta_{t,1}^2}} %\\
%%&\implies \frac{\sum_{i\geq 2} \delta_{t,i}^2}{\delta_{t,1}^2} \leq \epsilon^2 \leq \frac{\epsilon^2}{1 - \epsilon^2}
%\end{align*}
%and in fact, $\frac{\sum_{i \geq 2} \delta_{t,i}^2}{\delta_{t,1}^2} \leq \epsilon^2$ suffices since $\epsilon^2 \leq \frac{\epsilon^2}{1 - \epsilon^2}$. 
%
We need to treat the case of $\lambda_i <1 $ and $\lambda_i \geq 1$ separately. The justification for this is provided in Sec. \ref{sec:expl}. To this end, define $2 \leq i_\ind \leq n$, such that for all $i \leq i_\ind$, $\lambda_i \geq 1$ and for $i > i_\ind$, $\lambda_i < 1$. Furthermore, for $i \geq 2$, let $\nu_i = \lambda_i / \lambda_2$ and is easy to see that $\nu_i \leq 1$. Said another way, we can bound the smaller eigenvalues as follows
\begin{align*}
\sum_{i \geq 2} \lambda_i = \lambda_2 \sum_{i\geq 2} \nu_i  := \nu \lambda_2
\end{align*}

For notational simplicity, define $R_i = \lambda_i/\lambda_1$ and $\nsrmax = \sigma_c/\lambda_1$. Thus we have

\begin{align*}
\tan^2\theta(u_1^*, u_t)  &\leq C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} = C_\gamma \sum_{i\geq 2}  \frac{ \lambda_i^{2t} + \sigma_c^2(1 + \cdots + \lambda_i^{2t-2}) }{\lambda_1^{2t} + \sigma_c^2(1 + \cdots + \lambda_1^{2t-2})} \\
&= C_\gamma \sum_{i\geq 2}  \frac{ R_i^{2t} + \sigma_c^2\lambda_1^{-2t}(1 + \cdots + \lambda_i^{2t-2}) }{1 + \sigma_c^2\lambda_1^{-2t}(1 + \cdots + \lambda_1^{2t-2})} \\
&\leq C_\gamma \sum_{i\geq 2} \left[ R_i^{2t} +  \frac{\sigma_c^2}{\lambda_1^2} \cdot \frac{1 + \cdots + \lambda_i^{2t-2}}{\lambda_1^{2t-2}} \right] \\
&\leq C_\gamma \sum_{i\geq 2} \left[ R_i^{2t} +  \nsrmax^2 t \cdot \frac{\max(1, \lambda_i)^{2t-2}}{\lambda_1^{2t-2}} \right] \\
&\leq C_\gamma \sum_{i\geq 2}^{i_\ind} \left[R_i^{2t} +  \nsrmax^2 t \cdot R_i^{2t-2}\right] + C_\gamma \sum_{i > i_\ind} \left[\left(\frac{1}{\lambda_1}\right)^{2t} +  \nsrmax^2 t \cdot \left(\frac{1}{\lambda_1}\right)^{2t-2}\right] \\
&\leq C_\gamma R_2^{2t-2} \sum_{i\geq 2}^{i_\ind}  \left[ \nu_i^{2t} R_2^{2} +  \nu_i^{2t-2}\nsrmax^2 t \right] + C_\gamma \left(\frac{1}{\lambda_1}\right)^{2t-2} \sum_{i > i_\ind} \left[\left(\frac{1}{\lambda_1}\right)^{2} +  \nsrmax^2 t \cdot\right] \\
&\leq C_\gamma t \nu R_2^{2t-2} \left( R_2^2 + \nsrmax^2\right) + C_\gamma t (n - i_\ind) \left(\frac{1}{\lambda_1}\right)^{2t-2} \left(\left(\frac{1}{\lambda_1}\right)^{2} +  \nsrmax^2 \right) 
\end{align*}
And thus as long as 
\begin{align*}
t - 1 \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu t (R_2^2 + \nsrmax^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma} (n - i_\ind) t (1 /\lambda_1^2 + \nsrmax^2)}{\epsilon}\right) \right\}
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.



{\color{teal} 
\subsection{A possible explanation for discrepancy}\label{sec:expl}
This may seem counter intuitive, but the reason is that at any given iteration, we only have access to noisy matrix vector products. The intuitive reasoning is as follows: at iteration $t-1$, the component of the algorithm estimate along the first eigenvector is $O( \lambda_1^{t-1} + \sigma_c^2 \lambda_1^{t-2})$ and the component along the second eigenvector is $O( \lambda_2^{t-1} + \sigma_c^2 \lambda_2^{t-2})$. When $\lambda_2 \geq 1$ obviously the first term dominates, and we are able to retain the convergence rate of $\lambda_2/\lambda_1$, but when $\lambda_2 <1$, without any extra information on $\sigma_c$ (which we do not place) it is likely that the second term dominates; tracing this recursion all the way back to the first estimate in fact we conclude that when $\lambda_2 < 1$, the component along the second direction is $O(\sigma_c^2)$! 

Another possible way to interpret this would be considering the small noise v.s. large noise sort of regime that is standard in statistical literature. What we are saying is that in fact, we can recover the true eigenvector even in presence of extremely large noise, but just that the convergence is slower. This is most likely due to the fact that the noise is isotropic, but still. 

We provide a quick numerical expriment to verify our claims in Fig. \ref{fig:chk_r1}, \ref{fig:chk_r2}

\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_1_5_l2_1_051_sig_10.eps}
\captionof{figure}{$\lambda_1 = 2.25$, $\lambda_2 = 1.1025$, $\sigma_c = 10$}
\label{fig:chk_r1}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_1_5_l2_0_051_sig_10.eps}
\captionof{figure}{$\lambda_1 = 2.25$, $\lambda_2 = 0.0025$, $\sigma_c = 10$. Even though $\lambda_2/\lambda_1$ is smaller than the previous case, the convergence is slower. Everything else is the same.}
\label{fig:chk_r2}
\end{minipage}
}


{\noindent \color{blue} not sure what is the best way to simplify -- $log t$ is technically not a problem, but is it possible to eliminate this dependence? -- seems like it is not possible without additional information on $\lambda_i$'s, but try more!}


 %Now, suppose that for $i \geq 2$, $\lambda_i \leq \nu_i \lambda_2$ for some $\nu_i \leq 1$ (this is not really an assumption, it holds trivially for $\nu_i = 1$). Said another way, we can bound the tail as follows
%\begin{align*}
%\sum_{i \geq 2} \lambda_i \leq \left(\sum_{i\geq 2} \nu_i \right)\lambda_2 := \nu \lambda_2
%\end{align*}
%Then, 
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \sum_{i \geq 2} \nu_i^{2t} \leq  C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu \leq C_\gamma (1 + t \sigma_c^2) \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu
%\end{align*}
%in the worst case, $\nu = n -1$ and in most practical cases, we can actually upper bound $\nu$ by a small numerical constant. 
%
%
%
%And thus as long as 
%%Now the trivial approach to simplifying the above approach is to upper bound all terms by $\lambda_2$. In that case, we will get the following expression
%%\begin{align*}
%%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq n C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} \frac{1 + t \nsrmin^2}{1 + \nsrmax^2} 
%%\end{align*}
%%
%%For sake of simplicity (in this general case), we define $\nsrmin = \frac{\sigma_c}{\min(\lambda_2, \lambda_2^{t})}$ which implies that  as long as 
%\begin{align*}
%t \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu  \max(1, t \sigma_c^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right) \right\}
%\end{align*}
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.

%{\color{blue} Points to discuss:
%
%1.  what is the non-trivial method to bounding the above? -- don't think getting rid of $n$ is really possible, unless we know quantitative rate of decay of eigenvalues! 
%
%2. this is actually a problem when $\lambda_i \to 0$ since $\nsrmin \approx 1/\lambda_i^2$ can blow up. one possible fix is to rewrite the 1 + nsr quantity in terms of condition number} 

%We will now derive the explicit dependence of $\sigma_c$ on the number of iterations by manipulating the following expression
%%\newcommand{\}{â€¢}
%\begin{align*}
%C_\sigma &= \frac{\lambda_1^2}{\lambda_1^2 + \sigma_c^2} \cdot \frac{\lambda_2^2 - 1 + \sigma_c^2}{\lambda_2^2-1} \\
%&= \frac{1}{1 + \nsrmax^2} \cdot \frac{\lambda_2^2 - 1 + \sigma_c^2}{\lambda_2^2-1}
%\end{align*}
%For sake of simplicity, we upper bound the first term by $1$, and observe that as long as 
%\begin{align*}
%\sigma_c^2 \leq C_1 \left[ \left(\frac{\lambda_2}{\lambda_1}\right)^2 - 1\right] \leq C_1 [\lambda_2^2 - 1]
%\end{align*}
%the second term in $c_\sigma \leq 1 + C_1$. (wrong!!!! r.h.s is negative)



\section{Rank-$1$, i.i.d. over time, Anisotropic covariance}
Now we assume that the channel noise, $w_t$ at each iteration is independent but is not isotropic. Concretely, we assume that for all $t$, $\E[w_t w_t{}'] = \Sigma_w$, and is independent of $A$ (if this is random), $u_0$ and all the previous $w_{1:t-1}$. As done in the isotropic setting, we can write the iterates as follows
\begin{align*}
u_t = A^t u_0 + \sum_{\tau = 1}^t A^{t-\tau} w_\tau
\end{align*}
x
We have $u_0 = \alpha_1 u_1^* + \alpha_2 u_2^* + \cdots + \alpha_n u_n^*$ with $\sum_i \alpha_i^2 = \|u_0\|_2^2$. Similarly, for all $t$, we write $w_t = \beta_{t,1} u_1^* + \beta_{t,2} u_2^* +  \cdots + \beta_{t,n}^* u_n^*$, and here too, $\sum_i \beta_{t,i}^2 = \|w_t\|_2^2$. 

Observe that the $\alpha_i$'s are zero-mean independent random variables, and $\E[\alpha_i^2] = 1$ and $\beta_{t,i} = (u_i^*)^T w_t$ is a Gaussian r.v. with $\E[\beta_{t,i}] = 0$ and $\E[\beta_{t,i}^2] = (u_i^*)^T \Sigma_w u_i^* := \sigma_{c,i}^2$. We must note that the $\beta_{t,i}$'s are not independent any more -- with respect to the second index, but are independent over time, and are independent of the $\alpha_i$'s. 

Again, as in the isotropic case, we have 
\begin{align*}
u_t &= \sum_{i=1}^n \left(\alpha_i \lambda_i^t + \sum_{\tau=1}^t  \beta_{\tau,i} \lambda_i^{t - \tau}\right) u_i^*  := \sum_{i=1}^n \delta_{t,i}  u_i^*
\end{align*}

Notice again that for any $t,i$, $\delta_{t,i}$ is a Gaussian r.v. since it is the linear combination of $t+1$ gaussian r.v.'s. Furthermore, $\E[\delta_{t,i}] = 0$ and 
\begin{align*}
\E[\delta_{t,i}^2] = \E[(\alpha_i \lambda_i^t + \sum_{\tau=1}^t \beta_{\tau,i} \lambda_i^{t-\tau})^2] = \lambda_i^{2t} +  \sum_{\tau=1}^t \sigma_{c,i}^2 \lambda_i^{2(t-\tau)} = \lambda_i^{2t} + \sigma_{c,i}^2 \frac{\lambda_i^{2t}-1}{\lambda_i^2-1} := \sigma_{t,i}^2
\end{align*}
where we define $\sigma_{c,i} = (u_i^*)^T \Sigma_w u_i^*$. As before, $Y_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$ is a zero-mean sub exponential r.v. with $\|Y_{t,i}\|_{\psi_1} \leq 2 \sigma_{t,i}^2$. However, since the $Y_{t,i}$'s are correlated, we do not use a tail bound for sum of exponential r.v but instead just bound the failure probability for each r.v. followed by a union bound, and thus we have

\begin{align}\label{eq:del_bnd}
\Pr\left( |Y_{t,i} \geq \epsilon_{t,i}| \right) \leq \exp\left[-c\min\left(\frac{\epsilon_{t,i}^2}{\sigma_{t,i}^4}, \frac{\epsilon_{t,i}}{\sigma_{t,i}^2} \right)\right]
\end{align}

%\begin{align*}
%\Pr\left( \bigg|\sum_i X_{t,i} \bigg| \geq \epsilon^* \sum_i \sigma_{t,i}^2 \right) \leq 2 \exp\left[ -c \min \left( \frac{(\epsilon^*)^2 (\sum_{i} \sigma_{t,i}^2)^2}{\sum \sigma_{t,i}^4}, \frac{\epsilon^* \sum_i \sigma_{t,i}^2}{\sigma_{t,1}^2} \right) \right]
%\end{align*}
and for $i \geq 1$, we use an upper bound, and we set $\epsilon_{t,i} = ({\gamma}_1 + \log n) \sigma_{t,i}^2$ for some ${\gamma}_1$. We notice that the first term dominates in the above probability expression so that we get a failure probability of $2 \exp(-c \gamma_1^2)/n$ {\color{blue} (we also mention that this increases the number of iterations by a factor of $\log \log n$ which is not problematic)}. And for $i=1$, we use the lower bound and we set $\epsilon_{t,1} = \gamma_0 \sigma_{t,1}^2$ and thus we have


%Now, for some $\gamma \in (0,1)$, set $\epsilon_{t,1} = \gamma_0 \sigma_{t,1}^2$ and $\epsilon_{t,2} = \gamma_1 \sigma_{t,2}^2$.
Thus, 
\begin{align*}
&\Pr\left( \delta_{t,1}^2 \geq (1 - \gamma_0) \sigma_{t,1}^2 \right) \geq 1 - \exp(-c \gamma_0^2) \\ 
&\Pr\left( \sum_{i =2}^n \delta_{t,i}^2 \leq (1 + \gamma_1 + \log n) \sum_{i=2}^n \sigma_{t,i}^2 \right) \geq 1 - \sum_{i\geq 2} \exp(-c (\gamma_1 + \log n)^2) \geq 1 - \exp(-c {\gamma}_1^2) \\ 
\end{align*}

And thus, with probability at least $ 1 - \exp(-c \gamma_0^2) - \exp(-c {\gamma}_1^2)$ %(also assume that $\lambda_1 > \lambda_2 \geq 1$),
\begin{align*}
\sum_{i \geq 2}\frac{\delta_{t,i}^2}{\delta_{t,1}^2} &\leq  \left(\frac{1 + \gamma_1 + \log n}{1 - \gamma_0}\right) \frac{\sum_{i\geq 2} \sigma_{t,2}^2}{\sigma_{t,1}^2} = C_{\gamma,n} \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2}
\end{align*}
We use two subscripts in $C_{\gamma,n}$ to denote the dependence on $n$.
Notice that
\begin{align*}
\tan^2\theta(u_t, u_1^*) = \frac{\sum_{i\geq 2}  \delta_{t,i}^2}{\delta_{t,1}^2}
\end{align*}
and since $\sin\theta \leq \tan \theta$, it suffices to show that $\tan^2\theta \leq \epsilon^2$.

As done in the isotropic case, define $2 \leq i_\ind \leq n$, such that for all $i \leq i_\ind$, $\lambda_i \geq 1$ and for $i > i_\ind$, $\lambda_i < 1$. Furthermore, for $i \geq 2$, let $\nu_i = \lambda_i / \lambda_2$ and is easy to see that $\nu_i \leq 1$. Said another way, we can bound the smaller eigenvalues as follows
\begin{align*}
\sum_{i \geq 2} \lambda_i = \lambda_2 \sum_{i\geq 2} \nu_i  := \nu \lambda_2
\end{align*}

For notational simplicity, define $R_i = \lambda_i/\lambda_1$. Thus we have
\begin{align*}
\tan^2\theta(u_1^*, u_t) &\leq C_{\gamma,n} \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} 
\leq C_{\gamma,n} \sum_{i\geq 2}  \frac{ \lambda_i^{2t} + \sigma_{c,i}^2(1 + \cdots + \lambda_i^{2t-2}) }{\lambda_1^{2t} + \sigma_{c,i}^2(1 + \cdots + \lambda_1^{2t-2})} \\
&= C_{\gamma,n} \sum_{i\geq 2}  \frac{ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} + \sigma_{c,i}^2 \lambda_1^{-2t}(1 + \cdots + \lambda_i^{2t-2}) }{1 + \sigma_{c,1}^2\lambda_1^{-2t}(1 + \cdots + \lambda_1^{2t-2})}\\
&\leq C_{\gamma,n} \sum_{i\geq 2} R_i^{2t} +  \frac{\sigma_{c,i}^2 \lambda_1^{-2t}(1 + \cdots + \lambda_i^{2t-2}) }{\max(1, \sigma_{c,1}^2 \lambda_1^{-2})}\\
&\leq C_{\gamma,n} \sum_{i\geq 2} \left[ R_i^{2t} + \min\left( \frac{\sigma_{c,i}^2}{\lambda_1^2}, \frac{\sigma_{c,i}^2}{\sigma_{c,1}^2} \right) t \cdot \frac{\max(1, \lambda_i)^{2t-2}}{\lambda_1^{2t-2}} \right] \\
&= C_{\gamma,n} \sum_{i\geq 2} \left[ R_i^{2t} + \nsrmax_i^2 t \cdot \frac{\max(1, \lambda_i)^{2t-2}}{\lambda_1^{2t-2}} \right] \\
&\leq C_{\gamma,n} \sum_{i\geq 2}^{i_\ind} \left[ R_i^{2t} + \nsrmax_i^2 t R_i^{2t-2} \right] + C_{\gamma,n} \sum_{i > i_\ind} \left[ \left(\frac{1}{\lambda_1}\right)^{2t} + \nsrmax_i^2 t \cdot \left(\frac{1}{\lambda_1}\right)^{2t-2} \right] \\
&\leq C_{\gamma,n} t R_2^{2t-2} \sum_{i\geq 2}^{i_\ind} \left[ \nu_i^{2t} R_2^{2} + \nu_i^{2t-2} \max_i(\nsrmax_i)^2 \right] + C_{\gamma,n} t \left(\frac{1}{\lambda_1}\right)^{2t-2} \sum_{i > i_\ind} \left[ \left(\frac{1}{\lambda_1}\right)^{2} + \max_i(\nsrmax_i)^2 \right] \\
&\leq C_{\gamma,n} \nu t R_2^{2t-2} \left( R_2^{2} + \nsrmax^2 \right) + C_{\gamma,n} (n - i_\ind) t \left(\frac{1}{\lambda_1}\right)^{2t-2} \left( \left(\frac{1}{\lambda_1}\right)^{2} + \nsrmax^2 \right)
\end{align*}
where $\nsrmax_i := \min\left( \frac{\sigma_{c,i}^2}{\lambda_1^2}, \frac{\sigma_{c,i}^2}{\sigma_{c,1}^2} \right)$ and  $\nsrmax := \max_i \nsrmax_i$ is the proxy for noise to signal ratio. And thus as long as 
\begin{align*}
t - 1 \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma,n} \nu t (R_2^2 + \nsrmax^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma,n} (n - i_\ind) t (1 /\lambda_1^2 + \nsrmax^2)}{\epsilon}\right) \right\}
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.

%\subsection{Eigenvalues $\lambda_i < 1$}
%
%When $\lambda_i < 1$, we have a similar approach as done in the $2$ non-zero eigenvalue case. In particular, 
%
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} &\leq C_\gamma \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2} C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq 2}  \frac{\lambda_i^{2t} + \sigma_c^2 (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2}  \\
%&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq 2}  \left[\nu_i^{2t} \lambda_2^{2t} + \sigma_c^2 (\nu_i^{2t-2} \lambda_2^{2t-2} + \cdots + 1)\right] \\
%&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \left[ \lambda_2^{2t} \nu + \sigma_c^2 ( n + \nu (t-1)) \right]  \\
%&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t) \\
%\end{align*}
%And thus as long as 
%\begin{align*}
%t \geq C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right)
%\end{align*}
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.
%
%\subsection{Mixed setting}
%Now consider the case when for all $ i < i_\ind$ $\lambda_i \geq 1$ and for $i  \geq i_\ind$, $\lambda_i < 1$.
%
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2}  & \leq C_\gamma \sum_{i = 2}^{i_\ind -1} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2} +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\lambda_i^{2t} + \sigma_c^2 (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2}  \\
%&\leq C_\gamma  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  (1 + t\sigma_c^2) \sum_{i = 2}^{i_\ind -1}\nu_i^{2t}  +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\nu_i \lambda_2^{2t} + \sigma_c^2 (\lambda_2^{2t-2} \nu_i^{2t-2} + \lambda_2^{2t-4} \nu_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2} \\
%&\leq C_\gamma \nu  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \max(1, t\sigma_c^2) +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t) 
%\end{align*}
%And thus as long as 
%\begin{align*}
%t \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu  \max(1, t \sigma_c^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right) \right\}
%\end{align*}
%with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.


\section{High-Probability bounds for the general rank $r$ setting}
Now, we consider the case where we are interested in finding the top-$r$ eigenvectors of $A$. We still assume the same setting, i.e., $A = U^* \Lambda^* (U^*)^T$ and we want to compute $\hat{U}_t \in \R^{n \times r}$ that is a good estimate of $U = [u_1^*, \cdots, u_r^*]$. To simplify some notation, we will often use the following e.v.d. 
\begin{align*}
A = 
\begin{bmatrix}
U \\ U_{\perp}
\end{bmatrix}
\begin{bmatrix}
\Lambda & 0  \\
0 & \Lambda_{\perp}
\end{bmatrix}
\begin{bmatrix}
U \\ U_{\perp}
\end{bmatrix}^T
\end{align*}
 The algorithm for this is a slight modification of the rank-$1$ case, but we provide it here for simplicity. The main difference is that the normalization (which is not performed) is replaced with a reduced QR decomposition. 

\begin{algorithm}[H]
\caption{Noisy power method -- rank $r$}\label{algo:rankr}
  \begin{algorithmic}[1]
    \REQUIRE $A$ 
    \STATE $U_0 \overset{i.i.d.}{\sim} \mathcal{N}(0, I_{n \times r})$ %(Initialization)
    \STATE $U_0 R_0 \overset{QR}{\leftarrow} U_0$(step is excluded currently in analysis)
    \FOR{$t=1,\dots, T$}
    \STATE $W_t \overset{i.i.d.}{\sim} \mathcal{N}(0, \sigma_c^2 I_{n \times r})$
    \STATE $U_t = A U_{t-1} + W_t$ 
    \STATE $U_t R_t \overset{QR}{\leftarrow} U_t$(step is excluded currently in analysis)
    \ENDFOR
    \ENSURE $U_T$ 
  \end{algorithmic}
\end{algorithm}


\newcommand{\ucos}{\bm{\alpha}}
\newcommand{\usin}{\bm{\alpha}_{\perp}}

\newcommand{\wcos}{\bm{\beta}}
\newcommand{\wsin}{{\bm{\beta}_{\perp}}}

\newcommand{\utcos}{\bm{\delta}}
\newcommand{\utsin}{{\bm{\delta}_{\perp}}}


As done in the rank $1$ case, we have the following, 
\begin{gather*}
U_1 = A U_0 + W_1 \\
U_2 = A U_1 + W_2 = A^2 U_0 + A W_1 + W_2\\
\vdots \\
U_t = A^t U_0 + \sum_{\tau = 1}^t A^{t-\tau} W_\tau
\end{gather*}

Further, for any matrix $M$ we can decompose it along the desired subspace, and orthogonal to it as $M = U U^T M + U_{\perp} U_{\perp}^T M$ and thus, we write
\begin{gather*}
U_0 = U U^T U_0 + U_{\perp} U_{\perp}^T U_0 := U \ucos + U_{\perp} \usin \\
W_t = U U^T W_t + U_{\perp} U_{\perp}^T W_t := U \wcos_t + U_{\perp} \wsin_t 
\end{gather*}
Additionally, 
\begin{align*}
U_t &= A^t (U \ucos + U_{\perp} \usin) + \sum_{\tau = 1}^t A^{t-\tau} (U \wcos_\tau + U_{\perp} \wsin_\tau) \\
&= U \Lambda^t \ucos + U_\perp \Lambda_\perp^t \usin + \sum_{\tau = 1}^t U \Lambda^{t-\tau} \wcos_\tau + U_\perp \Lambda_\perp^{t-\tau} \wsin_\tau \\
&= U\left( \Lambda^t \ucos + \sum_{\tau=1}^t \Lambda^{t-\tau} \wcos_\tau \right) + U_\perp \left( \Lambda_\perp^t \usin + \sum_{\tau=1}^t \Lambda_\perp^{t-\tau} \wsin_\tau \right) \\
&:= U \utcos_t + U_\perp \utsin_t
\end{align*}

Furthermore, notice that $\ucos = U^T U_0 \overset{i.i.d.}{\sim} \mathcal{N}(0, I_{r \times r})$, $\usin = U_\perp^T U_0 \overset{i.i.d.}{\sim} \mathcal{N}(0, I_{n- r \times r})$, $\wcos_t = U^T W_t \overset{i.i.d.}{\sim} \mathcal{N}(0, \sigma_c^2 I_{r \times r})$, $\wsin_t = U_\perp^T W_t \overset{i.i.d.}{\sim} \mathcal{N}(0, \sigma_c^2 I_{n- r \times r})$ and they are all independent random matrices. 
%We can write the expression for $u_t$ as follows
%\begin{align*}
%u_t &= A^t u_0 + \sum_{\tau=1}^t A^{t - \tau} w_\tau \\
%&= A^t \left( \sum_{i=1}^n \alpha_i u_i^* \right) + \sum_{\tau=1}^t A^{t - \tau} \left( \sum_{i=1}^n \beta_{\tau,i} u_i^* \right) \\
%&= \sum_{i=1}^n \alpha_i (A^t u_i^*)  + \sum_{\tau=1}^t \sum_{i=1}^n \beta_{\tau,i} (A^{t - \tau} u_i^*)  \\
%&= \sum_{i=1}^n \alpha_i (\lambda_i^t u_i^*)  + \sum_{\tau=1}^t \sum_{i=1}^n \beta_{\tau,i} (\lambda_i^{t - \tau} u_i^*)  \\
%&= \sum_{i=1}^n \alpha_i \lambda_i^t u_i^*  + \sum_{i=1}^n \sum_{\tau=1}^t  \beta_{\tau,i} \lambda_i^{t - \tau} u_i^*  \\
%&= \sum_{i=1}^n \left(\alpha_i \lambda_i^t + \sum_{\tau=1}^t  \beta_{\tau,i} \lambda_i^{t - \tau}\right) u_i^*  \\
%&:= \sum_{i=1}^n \delta_{t,i}  u_i^*
%\end{align*}

For computing the subspace error in the rank $r$ case we need to first construct a basis matrix for $U_t$ and we do through the reduced QR decomposition. Let $U_t \overset{QR}{=} Q_t R_t$ where $Q_t \in \R^{n \times r}$ is a basis matrix, and $R_t \in \R^{r \times r}$ is an upper triangular matrix with the property that $\sigma_i(R_t) = \sigma_i(U_t)$ for all $ i \in [r]$. Recall that $\utsin_t \in \R^{n -r \times r}$ and $\utcos_t \in \R^{r \times r}$. 
\begin{align*}
\SE(U, U_t) = \|U_\perp U_\perp^T Q_t\| = \|U_\perp U_\perp^T U_t R_t^{-1}\|  = \|U_\perp \utsin_t R_t^{-1}\| = \|\utsin_t R_t^{-1}\| \leq \frac{\|\utsin\|}{\sigma_{r}(R_t)}
\end{align*}
Also,
\begin{align*}
\sigma_r^2(R_t) = \sigma_{r}^2(U_t) = \lambda_{\min}(U_t^T U_t) = \lambda_{\min}(\utcos_t^T \utcos_t + \utsin_t^T \utsin_t) \geq \lambda_{\min}(\utcos_t^T \utcos_t) + \lambda_{\min}(\utsin_t^T \utsin_t) \geq \lambda_{\min}(\utcos_t^T \utcos_t) = \sigma_r^2(\utcos_t)
\end{align*}
and thus we have 
\begin{align*}
\SE^2(U_,U_t) \leq \frac{\|\utsin_t\|_2^2}{\sigma_r^2(\utcos_t)}
\end{align*}

The intuition about why this works is that the denominator scales as (roughly) $\lambda_{r+1}^{2t}$ and the numerator scales as $\lambda_{r}^{2t}$. 

\subsection{Appropriate Concentrations}

{\color{blue} First we provide a naive analysis -- only union bound.  }

Consider the numerator 
\begin{align*}
\|\utsin_t\| &= \norm{\Lambda_\perp^t \usin + \sum_{\tau=1}^t \Lambda_\perp^{t-\tau} \wsin_\tau} \\
&\leq \norm{\Lambda_\perp^t} \norm{\usin} + \sum_{\tau=1}^t \norm{\Lambda_\perp^{t-\tau}} \norm{\wsin_\tau} \\
& = \lambda_{r+1}^t \norm{\usin} + \sum_{\tau=1}^t \lambda_{r+1}^{t - \tau} \norm{\wsin_\tau}
\end{align*}
Now, we use the following result \cite[Theorem 4.4.5]{hdp_book} 
\begin{theorem}
Let $A$ be a $m \times n$ random matrix whose entries are independent zero-mean sub-Gaussian r.v.'s. Then for any $t >0$ we have 
\begin{align*}
\|A\| \leq C K (\sqrt{m} + \sqrt{n} + t) 
\end{align*}
with probability at least $1 - 2 \exp(-t^2)$. Here, $K = max_{i,j} \|A_{i,j}\|_{\psi_2}$
\end{theorem}
Firstly notice that $m \equiv n-r$ and $n \equiv r$. Also, for $\usin$, $K = 1$ and for $\wsin_t$, $K = \sigma_c$. Thus, 
\begin{align*}
\Pr\left(\|\usin\| \leq C (1 + \gamma_1)(\sqrt{n-r} + \sqrt{r})\right) \geq  1 - \exp(- \gamma_1^2 n) \\
\Pr\left(\|\wsin_\tau\| \leq C \sigma_c (1 + \gamma_1)(\sqrt{n-r} + \sqrt{r}) \right) \geq  1 - \exp(- \gamma_1^2 n)
\end{align*}
where we used $t = \gamma_1 (\sqrt{n-r} + \sqrt{r}) \implies \|A\| \leq CK (1+ \gamma_1) (\sqrt{n-r} + \sqrt{r}) \leq 2CK (1+ \gamma_1) \sqrt{n}$ 

For simplicity (will work out the  details in some time) we will use a bound of $\sqrt{n}$ in the upper bound of the matrices. And thus, applying union bound, we notice that with probability at least $1 - (t + 1) \exp(- \gamma_1^2 n)$, 
\begin{align*}
\|\utsin_t\| \leq C (1+ \gamma_1) \sqrt{n} \left[\lambda_{r+1}^t +  \sigma_c (1 + \lambda_{r+1} + \cdots +  \lambda_{r+1}^{t-1}) \right]
\end{align*}

Now consider the denominator
\begin{align*}
\sigma_r(\utcos_t) &= \sigma_r\left(\Lambda^t \ucos + \sum_{\tau=1}^t \Lambda^{t-\tau} \wcos_\tau\right) \\
&\overset{a}{\geq}  \sigma_r\left(\Lambda^t \ucos\right) + \sum_{\tau=1}^t \sigma_r\left(\Lambda^{t-\tau} \wcos_\tau\right) \\
&\overset{b}{\geq}  \sigma_r\left(\Lambda^t \right) \sigma_r \left( \ucos\right) + \sum_{\tau=1}^t \sigma_r\left(\Lambda^{t-\tau}\right) \sigma_r\left(\wcos_\tau\right) \\
&= \lambda_r^t \sigma_r\left( \ucos\right) + \sum_{\tau=1}^t \lambda_{r}^{t-\tau} \sigma_r\left(\wcos_\tau\right) 
\end{align*}
where $(a)$ uses Weyl's inequality, $(b)$ follows because of Ostrowski's inequality as follows: 
\begin{align*}
\sigma_{\min}^2(AB) = \lambda_{\min}(B'A'A B) \geq \lambda_{\min}(A'A) \lambda_{\min}(B'B) = \sigma_{\min}^2(A) \sigma_{\min}^2(B)
\end{align*}
We can apply Ostrowski because we have that $B$ is non-singular, and $A'A$ is Hermitian. We now bound the smallest singular value (these are matrices of dimension $r$, recall) using the following result \cite[Theorem 1.2]{smallest}
\begin{theorem}
Let $A$ be a $n \times n$ random matrix whose entries are independent zero-mean sub-Gaussian r.v.'s. Then for any $t >0$ we have 
\begin{align*}
\sigma_{\min}(A) \geq \frac{t K}{\sqrt n}
\end{align*}
with probability at least $1 -  \exp(-c n) - (ct)$. Here, $K = max_{i,j} \|A_{i,j}\|_{\psi_2}$. 
\end{theorem}
We apply this result to $\ucos$ and $\wcos$. Firstly notice that $n \equiv r$. Also, we recall that for $\ucos$, $K = 1$ and for $\wcos_t$, $K = \sigma_c$. Thus
\begin{align*}
\Pr\left(\sigma_{r}(\ucos) \geq C \gamma_0/\sqrt{r}\right) \geq  1 - \exp(-cr) - (c \gamma_0) \\
\Pr\left(\sigma_{r}(\wcos_t) \geq C \gamma_0/\sqrt{r}\right) \geq  1 - \exp(-cr) - (c \gamma_0)
\end{align*}
where we set $t = \gamma_0$ and notice that the matrices are all $r$-dimensional. And thus, with probability at least $1 - (t+1) (\exp(-cr) + (c\gamma_0))$, 
\begin{align*}
\sigma_r(\utcos_t) \geq C \gamma_0/\sqrt{r} \left[ \lambda_r^t + \sigma_c (1 + \lambda_r + \cdots + \lambda_r^{t-1})\right]
\end{align*}
Finally, we get that with high probability, 
and thus we have 
\begin{align*}
\SE(U_,U_t) &\leq \frac{\|\utsin_t\|_2}{\sigma_r(\utcos_t)} \leq \frac{C (1+ \gamma_1) \sqrt{n} \left[\lambda_{r+1}^t +  \sigma_c (1 + \lambda_{r+1} + \cdots +  \lambda_{r+1}^{t-1}) \right]}{C \gamma_0/\sqrt{r} \left[ \lambda_r^t + \sigma_c (1 + \lambda_r + \cdots + \lambda_r^{t-1})\right]} \\
&= \frac{C (1+ \gamma_1) \sqrt{nr}}{\gamma_0} \cdot \frac{\lambda_{r+1}^t +  \sigma_c (1 + \lambda_{r+1} + \cdots +  \lambda_{r+1}^{t-1})}{\lambda_r^t + \sigma_c (1 + \lambda_r + \cdots + \lambda_r^{t-1})} \\
&:= C_{\gamma,n} \frac{\lambda_{r+1}^t +  \sigma_c (1 + \lambda_{r+1} + \cdots +  \lambda_{r+1}^{t-1})}{\lambda_r^t + \sigma_c (1 + \lambda_r + \cdots + \lambda_r^{t-1})}\\
&\leq C_{\gamma,n} \frac{\left(\frac{\lambda_{r+1}}{\lambda_r}\right)^t +  \sigma_c \lambda_r^{-t} (1 + \lambda_{r+1} + \cdots +  \lambda_{r+1}^{t-1})}{1 + \sigma_c \lambda_r^{-t} (1 + \lambda_r + \cdots + \lambda_r^{t-1})} \\
&\leq C_{\gamma,n} \left(\frac{\lambda_{r+1}}{\lambda_r}\right)^t + \frac{\sigma_c}{\lambda_r} t \frac{\max(1, \lambda_{r+1})^{t-1}}{\lambda_{r+1}^{t-1}}\\
&\leq C_{\gamma,n} t \left( \frac{\max(1, \lambda_{r+1})}{\lambda_{r}}\right)^{t-1} \left[ \left(\frac{\max(1,\lambda_{r+1})}{\lambda_r}\right) + \frac{\sigma_c}{\lambda_r} \right] \\
&\overset{(a)}{\leq} C_{\gamma,n} t \left( \frac{\lambda_{r+1}}{\lambda_{r}}\right)^{t-1} \left[ \left(\frac{\lambda_{r+1}}{\lambda_r}\right) + \frac{\sigma_c}{\lambda_r} \right]
\end{align*}

wherein, for simplicity, we assume in $(a)$ that $\lambda_{r+1} > 1$, then we have the following conclusion:

\begin{theorem}[Main Result -- Isotropic Noise]
Consider Algorithm \ref{algo:rankr}.
Define $R_1 = \lambda_{r+1}/\lambda_r$ and $\nsrmax = \sigma_c/\lambda_r$. 
Assume that the noise at each iteration is independent, and isotropic. Pick an $\epsilon > 0$. Also assume that $\lambda_r > \lambda_{r+1} \geq 1$.  Assume that the number of iterations, $t > t^* - 1$ with
\begin{align*}
t^* = C \frac{1}{\log(1/R_1)} \log   \left( \frac{C_{\gamma,n} t (R_1 + \nsrmax)}{\epsilon}\right) = C \frac{1}{\log(1/R_1)} \log   \left( \frac{\frac{(1+ \gamma_1)\sqrt{nr}}{\gamma_0} t (R_1 + \nsrmax)}{\epsilon}\right)
\end{align*}
 Then, with probability at least $1 - (t + 1) \exp(- \gamma_1^2 n) - (t+1) (\exp(-cr) + (c\gamma_0))$, the estimate satisfies
 \begin{align*}
 \SE(U_t, U) \leq \epsilon
 \end{align*}
\end{theorem}

\color{black}
\bibliographystyle{apalike}
\bibliography{numerical_analysis}

\clearpage
\null
\clearpage

%\null \clearpage

%\newcommand{\SE}{\mathrm{SE}}
\section{Numerical Experiments}
In this section we will describe the results of the numerical expreiments on synthetic data. We generate the data matrix $A = \sum_{i=1}^2 \lambda_1 u_i^* (u_i^*)^T$, where $u_i^*$ are columns of a orthnormalized i.i.d. Gaussian matrix. $A$ is a rank-$2$ matrix with a ``eigengap'' of $\lambda_2/\lambda_1$. We generate the ''channel noise'' $w_t$ at each iteration as $w_t \overset{iid}{\sim} \mathcal{N}(0, \sigma_c^2 I_n)$. We present the results below for various values of $\lambda_1, \lambda_2, \sigma_c$. 

We try two different variants of the power method. In the first we normalize the estimate $u_t$ at each iteration which we refer to as {\em PM with normalization} and the second where we do not normalize, and refer to this as {\em PM without normalization}. We plot the subspace error, $\SE(u_t, u_1^*)$ w.r.t the iteration, $t$. For the purpose of comparison, in each experiment, we also plot the subspace error for $\sigma_c = 0$, and refer to this case as {\em noiseless}. 

We see from Figs. \ref{fig:l1g1_1}, \ref{fig:l1g1_2} that in the case of $\lambda_1 > 1$, the normalized method does not work when the channel noise energy, $\sigma_c$ is large. However, considering only the un-normalized case in the sequel, notice that even if we set $\sigma_c \gg \lambda_1$, the algorithm converges to the true solution, but requires more number of iterations.

\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.5]{figures/l1_10_l2_5e-1_sig_10.eps}
\captionof{figure}{$\lambda_1 = 100$, $\lambda_2 = 0.25$, $\sigma_c^2 = 100$}
\label{fig:l1g1_1}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_125e-2_l2_5e-1_sig_10.eps}
\captionof{figure}{$\lambda_1 = 2.25$, $\lambda_2 = 0.25$, $\sigma_c^2 = 100$}
\label{fig:l1g1_2}
\end{minipage}

However, when we have $\lambda_1 \leq 1$, we do not observe convergence. We show the results for this case in Figs. \ref{fig:l1e1_1}, \ref{fig:l1l1_1}. Notice that in this setting, Both methods fail to converge, but the normalized verison is slightly better in this case as compared to $\lambda_1 > 1$ case, but it still does not work. Furthermore, increasing the channel noise energy $\sigma_c$ does not significantly alter the convergence of either algorithm, but we do not show that comparison here. 

\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_1_l2_25e-41_sig_1e-6.eps}
\captionof{figure}{$\lambda_1 = 1$, $\lambda_2 = 2.5 \times 10^{-3}$, $\sigma_c^2 = 10^{-6}$}
\label{fig:l1e1_1}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.55]{figures/l1_81e-2_l2_25e-41_sig_1e-6.eps}
\captionof{figure}{$\lambda_1 = 0.81$, $\lambda_2 = 2.5 \times 10^{-3}$, $\sigma_c^2 = 10^{-6}$}
\label{fig:l1l1_1}
\end{minipage}

Finally, to alleviate the issue of $\lambda_1 \leq 1$, we consider the case of scaling the entries of the matrix $A$ to ensure that $\lambda_1 > 1$. In particular, we divide each entry by $2\lambda_1$. The results for this are presented in Fig. \ref{fig:l1e1_1_n}, \ref{fig:l1l1_1_n}. All the other parameters are the same as in the case of $\lambda_1 \leq 1$. In this case, as expected, the un-normalized power method is able to recover the true direction accurately. 

\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_1_l2_25e-41_sig_1e-6_norm.eps}
\captionof{figure}{$\lambda_1 = 1$, $\lambda_2 = 2.5 \times 10^{-3}$, $\sigma_c^2 = 10^{-6}$}
\label{fig:l1e1_1_n}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[scale=.6]{figures/l1_81e-2_l2_25e-41_sig_1e-6_norm.eps}
\captionof{figure}{$\lambda_1 = 0.81$, $\lambda_2 = 2.5 \times 10^{-3}$, $\sigma_c^2 = 10^{-6}$}
\label{fig:l1l1_1_n}
\end{minipage}

\section{Literature Survey}
First we describe the problem setting of several related problems. 

\subsection{Problem Settings}
\subsubsection{Streaming PCA}
In this problem, the goal is to compute the top singular vectors of a data stream. Typically, the data is assumed to be drawd i.i.d. from some ``nice'' distribution, and the constraint is that (i) one cannot store all the data in memory; (ii) one cannot make too many passes on the data.

Here we summarize the main results of some prominent papers in this area. 

{\bf Noisy Power method \cite{noisy_pm}:} To the best of our knowledge, this was one of the first papers to provide a robust convergence analysis of power method. In particular, their results shows that (i) in the context of streaming PCA, 

\color{blue}
\section{Discussion}
\begin{enumerate}
\item Note that in fact, if using naive bounds, in fact $C \approx n - 1$. maybe there is a better way to bound this which leverages independence etc. 
\item not a fully formed thought, but are there any connections between this and ``convergence of proximal algorithms'' for convex objectives?
\item this is not really convergence of output, it says normalized vector is close to ground truth
\item how to explain the fact that $\lambda_2$ and not $\lambda_1$ requires assumptions? is there a way to combine the cases, and absorb noise?
\end{enumerate}

\subsection{points of discussion after meeting}

\begin{enumerate}
\item existing work in this area
\item variants of power method
\item can look at coloured noise, across \em{space and time}
\item security -- byzantine issues
\item any relation to momentum/proximal algorithm convergence?
\item extension to lrmc etc
\end{enumerate}





\end{document} 



{\color{teal}
\begin{theorem}
Let $u_1^*$ denote the top eigenvector of $A \in \R^{n \times n}$, and let $\lambda_1 > \lambda_2 \geq \lambda_3 \geq \cdots \geq \lambda_n \geq 0$ denote its eigenvalues. Suppose that Algorithm \ref{algo:rank1} is initialized with $u_0 \sim \mathcal{N}(0, I_{n \times n})$ and at each iteration, the ``channel noise'' is independent (of all previous noise, the initialization, $u_0$ and $A$, if $A$ is not deterministic) and such for all $t$ that $w_t  \sim \mathcal{N}(0, \sigma_c^2 I_{n \times n})$. Pick an $\epsilon^* \in (0,1)$. 
\begin{itemize}
\item Let $\lambda_2 > 1$. And let for all $t$ {\color{blue}(the $\sqrt{n}$ can be eliminated by considering the energy, as opposed to gap, will do that after exam)},

\begin{align*}
\sigma_c^2 \leq \sqrt{\frac{C_{\epsilon^*}}{n}} \left(\frac{\lambda_1}{\lambda_2}\right)^t \varepsilon
\end{align*}

Then, with probability at least $1 - 2 \exp(-c (\epsilon^*)^2)$, the algorithm estimate at iteration $t$, $u_t$ satisfies
\begin{align*}
\sin^2\theta(u_t, u_1^*) \leq \varepsilon^2
\end{align*}

%and if $t \geq C \frac{\log((1 + \sigma_c^2)/\epsilon^*)}{\log(\lambda_1/\lambda_2)}$, $\sin^2\theta(u_t, u_1^*) \leq 3 \epsilon^*$
{\color{blue} ignore for now
\item If $\lambda_2 \leq 1$. Then, with probability at least $1 - 2 \exp(-c (\epsilon^*)^2)$, the algorithm estimate at iteration $t$, $u_t$ satisfies
\begin{align*}
\sin^2\theta(u_t, u_1^*) \leq 2 \epsilon^* + C (1+\sigma_c^2) \left(\frac{\lambda_2}{\lambda_1}\right)^{2t}
\end{align*}
and if $t \geq C \frac{\log((1 + \sigma_c^2)/\epsilon^*)}{\log(\lambda_1/\lambda_2)}$, $\sin^2\theta(u_t, u_1^*) \leq 3 \epsilon^*$
}
\end{itemize}
\end{theorem}
}

{
\color{red}
Firstly, we notice that for any $t,i$, $\delta_{t,i}$ is a Gaussian r.v. since it is the linear combination of $t+1$ gaussian r.v.'s. Furthermore, $\E[\delta_{t,i}] = 0$ and 
\begin{align*}
\E[\delta_{t,i}^2] = \E[(\alpha_i \lambda_i^t + \sum_{\tau=1}^t \beta_{\tau,i} \lambda_i^{t-\tau})^2] = \lambda_i^{2t} + \sigma_c^2 \sum_{\tau=1}^t \lambda_i^{2(t-\tau)} = \lambda_i^{2t} + \sigma_c^2 \frac{\lambda_i^{2t}-1}{\lambda_i^2-1} := \sigma_{t,i}^2
\end{align*}

%{\color{red}
%Thus, using the standard tail bound for a Gaussian r.v., we know that 
%\begin{align}
%\Pr\left( |\delta_{t,i}| \geq \epsilon_{t,i} \right) \leq 2 \exp\left(-\frac{\epsilon_{t,i}^2}{\sigma_{t,i}^2}\right)
%\end{align}
%}


And thus, $Y_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$ is a zero-mean sub exponential r.v. with $\|Y_{t,i}\|_{\psi_1} \leq 2 \sigma_{t,i}^2$. Thus, using the tail bound for sub-exponential r.v.'s we know that

\begin{align}\label{eq:del_bnd}
\Pr\left( |Y_{t,i}| \geq \epsilon_{t,i} \right) \leq 2 \exp\left[-c\min\left(\frac{\epsilon_{t,i}^2}{\sigma_{t,i}^4}, \frac{\epsilon_{t,i}}{\sigma_{t,i}^2} \right)\right]
\end{align}


We will use \eqref{eq:del_bnd} with an appropriately defined $\epsilon_{t,i}$ below. Furthermore, $\delta_{t,i}$'s are zero-mean random variables. This is because all the $\alpha_i$'s and the  $\beta_{\tau,i}$'s are zero mean. Additionally, for $i \neq j$, $\E[\delta_{t,i} \delta_{t,j}] = \E[\delta_{t,i}] \E[\delta_{t,j}]$'s (they are actually independent). This is because, $u_0 \sim \mathcal{N}(0, I_{n \times n})$, $w_t \sim \mathcal{N}(0, \sigma_c^2 I_{n \times n})$, and also $w_t \indep w_{[1:t-1]},\ A,\ u_0$. Finally, $\delta_{t,i}^2$'s are sub-exponential (in fact $\chi_n^2$) r.v.'s since $\delta_{t,i}$'s are gaussian. 

Recall that $\|\delta_{t,i}^2\|_{\psi_1} = \|\delta_{t,i}\|_{\psi_2}^2 = \sigma_{t,i}^2$ and define $X_{t,i} = \delta_{t,i}^2 - \sigma_{t,i}^2$. We now use Bernstein's inequality to obtain
\begin{align}\label{eq:norm_bnd}
\Pr\left( \bigg|\sum_{i=1}^n X_{t,i}\bigg| > \epsilon_n \right) \leq 2 \exp\left[ -c \min\left( \frac{\epsilon_n^2}{\sum_i \sigma_{t,i}^4}, \frac{\epsilon_n}{\sigma_{t,1}} \right) \right]
\end{align}

First, we bound $\delta_{t,i}$ by setting
\begin{align*}
\epsilon_{t,i} = \epsilon^* \sigma_{t,i}^2 
\end{align*}
for some $\epsilon^* \in (0,1)$ we get using the above and \eqref{eq:del_bnd}
\begin{align*}
\Pr\left( \delta_{t,i}^2 \leq (1 - \epsilon^*) \sigma_{t,i}^2 \right) \leq 2 \exp\left( - c(\epsilon^*)^2 \right) 
\end{align*}

Now, we bound $\|u_t\|$ by setting 
\begin{align*}
\epsilon_n = \epsilon^* \sum_{i} \sigma_{t,i}^2
\end{align*}
and now using \eqref{eq:norm_bnd} we get
\begin{align*}
\Pr\left( \bigg|\sum_i X_{t,i} \bigg| \geq \epsilon^* \sum_i \sigma_{t,i}^2 \right) \leq 2 \exp\left[ -c \min \left( \frac{(\epsilon^*)^2 (\sum_{i} \sigma_{t,i}^2)^2}{\sum \sigma_{t,i}^4}, \frac{\epsilon^* \sum_i \sigma_{t,i}^2}{\sigma_{t,1}^2} \right) \right]
\end{align*}
and roughly (verify this), the first term dominates in the above so that we again get a failure probability of $2 \exp(-c(\epsilon^*)^2)$.


What these two mean is the following: with probability at least $1 - \exp( -c (\epsilon^*)^2 )$, 
\begin{align*}
\delta_{t,1}^2 \geq (1 - \epsilon^*) \sigma_{t,1}^2 \quad \text{and} \quad \|u_t\|^2 \leq (1 + \epsilon^*) \sum_i \sigma_{t,i}^2
\end{align*}
thus, with probability at least $1 - 2 \exp( -c (\epsilon^*)^2 )$, 
\begin{align*}
\sin^2\theta(u_t, u_1^*) \leq 1 - \frac{(1 - \epsilon^*) \sigma_{t,1}^2}{ (1 + \epsilon^*) \sum_i \sigma_{t,i}^2 } = 1 - C_{\epsilon^*} \frac{\sigma_{t,1}^2}{\sum_i \sigma_{t,i}^2}
\end{align*}
Now, we derive the requirement on $\sigma_c^2$ so that the right hand side above can be reduced to an $\varepsilon^2$ which implies $\varepsilon$-closeness to the true eigenvector. 

{\color{red} (First, we will use a loose bound and quantify this in terms of the eigengap, as opposed to the energy condition)}

Notice that if we can ensure 
\begin{align}\label{eq:suff_1}
\sum_{i} \frac{\sigma_{t,i}^2}{\sigma_{t,1}^2} = 1 + \sum_{i \geq 2} \frac{\sigma_{t,i}^2}{\sigma_{t,1}^2} \leq C_{\epsilon^*} \frac{1}{1-\varepsilon^2} 
\end{align}
 we will achieve $\varepsilon$-closeness. To this end, we derive the sufficient conditions. First consider the r.h.s. of \eqref{eq:suff_1}
\begin{align*}
\mathrm{r.h.s.} = C_{\epsilon^*} \frac{1}{1-\varepsilon^2}  - 1  = C_{\epsilon^*}\left(1 +  \frac{\varepsilon^2}{1-\varepsilon^2}\right)  - 1 = \frac{C_{\epsilon^*}\varepsilon^2}{1 - \varepsilon^2} + (C_{\epsilon^*} - 1) = \frac{C_{\epsilon^*}\varepsilon^2}{1 - \varepsilon^2} - \frac{2\epsilon^*}{1+\epsilon^*}
\end{align*} 
now, the practical regime is when $\varepsilon \ll \epsilon^*$ since $\varepsilon$ controls the final accuracy whereas $\epsilon^*$ controls the probability, and we can set $\epsilon =  0.8$, say so that the failure probability is $\approx 2 \exp(-0.8^2) \approx 0.1$ and thus
\begin{align*}
\mathrm{r.h.s.} \geq \frac{C_{\epsilon^*}\varepsilon^2}{2(1 - \varepsilon^2)} \geq 0.5 C_{\epsilon^*} \varepsilon^2
\end{align*}
and now, considering the l.h.s of \eqref{eq:suff_1}, 
\begin{align*}
\mathrm{l.h.s.} = \sum_{i \geq 2}  \frac{\sigma_{t,i}^2}{\sigma_{t,1}^2} = \sum_{i \geq 2} \frac{\lambda_i^{2t} + \sigma_c^2 \frac{\lambda_i^{2t}-1}{\lambda_i^2-1}}{\lambda_1^{2t} + \sigma_c^2 \frac{\lambda_1^{2t}-1}{\lambda_1^2-1}} \leq \sum_{i \geq 2} \frac{\lambda_i^{2t} + \sigma_c^2 \frac{\lambda_i^{2t}-1}{\lambda_i^2-1}}{\lambda_1^{2t}} \leq n \frac{\lambda_2^{2t} + \sigma_c^2 \frac{\lambda_2^{2t}-1}{\lambda_2^2-1}}{\lambda_1^{2t}}
\end{align*}
and thus,
\begin{align*}
\mathrm{l.h.s.} \leq 
\begin{cases}
n (1 + \sigma_c^2)  \left(\frac{\lambda_2}{\lambda_1}\right)^{2t}, \quad \lambda_2 \geq 1 \\
n \left(\frac{\lambda_2}{\lambda_1}\right)^{2t} + \sigma_c^2, \quad \lambda_2 < 1 \\
\end{cases}
\end{align*}
And thus we arrive at the following 
 \begin{align*}
&1 + n \cdot \frac{\lambda_2^{2t} + \sigma_c^2 (\lambda_2^{2t} -1)}{\lambda_1^{2t}} \approx C_{\epsilon^*}\left(1 + \frac{\varepsilon^2}{1 - \varepsilon^2}\right) \\
\implies& n \cdot \frac{(1+\sigma_c^2)\lambda_2^{2t}}{\lambda_1^{2t}} \approx \frac{C_{\epsilon^*} \varepsilon^2}{1 - \varepsilon^2} \\
  \end{align*}
and thus, if we assume that 
\begin{align*}
\sigma_c^2 \leq \sqrt{\frac{C_{\epsilon^*}}{n}} \left(\frac{\lambda_1}{\lambda_2}\right)^t \varepsilon
\end{align*}
we can achieve $\varepsilon$-closeness, with probability at least $1 - 2 \exp(-c(\epsilon^*)^2)$.
}


\section{Some old stuff}
In the worst case when $\lambda_i = \lambda_2$ for all $i \geq 2$,
as long as 
\begin{align*}
t - \log t -1 \geq C \frac{1}{\log(\lambda_1/\lambda_2)} \cdot \left[ \log   \left( \frac{C_{\gamma} n}{\epsilon}\right) - \log   \left( \frac{C_\gamma}{\nsrmax^2 + R_2^2}\right) \right]
\end{align*}

\subsubsection{Eigenvalues $\lambda_i \geq 1$}

First we consider the harder case when the smaller eigenvalues, $\lambda_i \geq 1$ and derive the dependence on the number or iterations required. 


%Now we consider two cases, (i) $\lambda_2  \geq 1$ and (ii)$\lambda_2 < 1$ and for these cases derive the dependence on the number of iterations reqired. 

\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} &\leq  C_\gamma \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2}  \\
&\leq C_\gamma \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + t \sigma_c^2 / \lambda_i^2 }{1 + \nsrmax^2}  \\
&\leq C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} %:=  C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} 
\end{align*}
where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio. 

Then, 
\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \sum_{i \geq 2} \nu_i^{2t} \leq  C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu \leq C_\gamma (1 + t \sigma_c^2) \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu
\end{align*}
in the worst case, $\nu = n -1$ and in most practical cases, we can actually upper bound $\nu$ by a small numerical constant. And thus as long as 
%Now the trivial approach to simplifying the above approach is to upper bound all terms by $\lambda_2$. In that case, we will get the following expression
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq n C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} \frac{1 + t \nsrmin^2}{1 + \nsrmax^2} 
%\end{align*}
%
%For sake of simplicity (in this general case), we define $\nsrmin = \frac{\sigma_c}{\min(\lambda_2, \lambda_2^{t})}$ which implies that  as long as 
\begin{align*}
t \geq C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu  \max(1, t \sigma_c^2)}{\epsilon}\right)
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$. 

\subsubsection{Eigenvalues $\lambda_i < 1$}

When $\lambda_i < 1$, we have a similar approach as done in the $2$ non-zero eigenvalue case. In particular, 

\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} &\leq C_\gamma \sum_{i \geq 2} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2} C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq 2}  \frac{\lambda_i^{2t} + \sigma_c^2 (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2}  \\
&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq 2}  \left[\nu_i^{2t} \lambda_2^{2t} + \sigma_c^2 (\nu_i^{2t-2} \lambda_2^{2t-2} + \cdots + 1)\right] \\
&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \left[ \lambda_2^{2t} \nu + \sigma_c^2 ( n + \nu (t-1)) \right]  \\
&\leq C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t) \\
\end{align*}
where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio.  %Now, suppose that for $i \geq 2$, $\lambda_i \leq \nu_i \lambda_2$ for some $\nu_i \leq 1$ (this is not really an assumption, it holds trivially for $\nu_i = 1$). Said another way, we can bound the tail as follows
%\begin{align*}
%\sum_{i \geq 2} \lambda_i \leq \left(\sum_{i\geq 2} \nu_i \right)\lambda_2 := \nu \lambda_2
%\end{align*}
%Then, 
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \sum_{i \geq 2} \nu_i^{2t} \leq  C_\gamma \frac{1 + t \sigma_c^2}{1 + \nsrmax^2} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu \leq C_\gamma (1 + t \sigma_c^2) \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \nu
%\end{align*}
%in the worst case, $\nu = n -1$ and in most practical cases, we can actually upper bound $\nu$ by a small numerical constant. 
%
%
%
And thus as long as 
%Now the trivial approach to simplifying the above approach is to upper bound all terms by $\lambda_2$. In that case, we will get the following expression
%\begin{align*}
%C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} \leq n C_{\gamma} \left( \frac{\lambda_2}{\lambda_1} \right)^{2t} \frac{1 + t \nsrmin^2}{1 + \nsrmax^2} 
%\end{align*}
%
%For sake of simplicity (in this general case), we define $\nsrmin = \frac{\sigma_c}{\min(\lambda_2, \lambda_2^{t})}$ which implies that  as long as 
\begin{align*}
t \geq C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right)
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.

\subsubsection{Mixed setting}
Now consider the case when for all $ i < i_\ind$ $\lambda_i \geq 1$ and for $i  \geq i_\ind$, $\lambda_i < 1$.

%When $\lambda_i < 1$, we have a similar approach as done in the $2$ non-zero eigenvalue case. In particular, 
{\color{red} older one, keeping for future if required
\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2}  & \leq C_\gamma \sum_{i = 2}^{i_\ind -1} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2} +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\lambda_i^{2t} + \sigma_c^2 (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2}  \\
&\leq C_\gamma  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  (1 + t\sigma_c^2) \sum_{i = 2}^{i_\ind -1}\nu_i^{2t}  +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\nu_i \lambda_2^{2t} + \sigma_c^2 (\lambda_2^{2t-2} \nu_i^{2t-2} + \lambda_2^{2t-4} \nu_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2} \\
&\leq C_\gamma \nu  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \max(1, t\sigma_c^2) +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t) 
\end{align*}
where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio. And thus as long as 
\begin{align*}
t \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu  \max(1, t \sigma_c^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right) \right\}
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.
}

\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2} &= C_\gamma \sum_{i\geq 2}  \frac{ \lambda_i^{2t} + \sigma_c^2(1 + \cdots + \lambda_i^{2t-2}) }{\lambda_1^{2t} + \sigma_c^2(1 + \cdots + \lambda_1^{2t-2})} \leq C_\gamma \sum_{i\geq 2}  \frac{ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} + \sigma_c^2 \lambda_1^{-2t}(1 + \cdots + \lambda_i^{2t-2}) }{1 + \sigma_c^2\lambda_1^{-2t}(1 + \cdots + \lambda_1^{2t-2})}\\
&\leq C_\gamma \sum_{i\geq 2} \left[ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} +  t \nsrmax^2 \frac{\max(1, \lambda_i^{2t-2})}{\lambda_1^{2t-2}} \right] \\
&\leq C_\gamma \sum_{i \geq 2}^{i_\ind-1} \left[ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} +  t \nsrmax^2 \frac{\lambda_i^{2t-2}}{\lambda_1^{2t-2}} \right] + C_\gamma \sum_{i\geq i_\ind} \left[ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} +  t \nsrmax^2 \frac{1}{\lambda_1^{2t-2}} \right] \\
&\leq C_\gamma  \left(\frac{\lambda_2}{\lambda_1}\right)^{2t}  \sum_{i \geq 2}^{i_\ind-1}\nu_i^{2t} +  t \nsrmax^2 \left(\frac{\lambda_2}{\lambda_1}\right)^{2t-2} \sum_{i \geq 2}^{i_\ind-1}\nu_i^{2t-2} + C_\gamma \sum_{i\geq i_\ind} \left[ \frac{\lambda_i^{2t}}{\lambda_1^{2t}} +  t \nsrmax^2 \frac{1}{\lambda_1^{2t-2}} \right] \\
&\leq C_\gamma \nu t \left(R_2^{2t} +  \nsrmax^2 R_2^{2t-2} + \nsrmax^2 \frac{1}{\lambda_1^{2t-2}}\right)
\end{align*}
where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio and $R_2 = \lambda_2/\lambda_1$.

In the worst case when $\lambda_i = \lambda_2$ for all $i \geq 2$,
as long as 
\begin{align*}
t - \log t -1 \geq C \frac{1}{\log(\lambda_1/\lambda_2)} \cdot \left[ \log   \left( \frac{C_{\gamma} n}{\epsilon}\right) - \log   \left( \frac{C_\gamma}{\nsrmax^2 + R_2^2}\right) \right]
\end{align*}

{\color{red} older one, keeping for future if required
\begin{align*}
C_\gamma \frac{\sum_{i\geq 2} \sigma_{t,i}^2}{\sigma_{t,1}^2}  & \leq C_\gamma \sum_{i = 2}^{i_\ind -1} \left( \frac{\lambda_i}{\lambda_1} \right)^{2t} \frac{1 + \sigma_c^2 \lambda_i^{-2t} (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots 1)}{1 + \nsrmax^2} +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\lambda_i^{2t} + \sigma_c^2 (\lambda_i^{2t-2} + \lambda_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2}  \\
&\leq C_\gamma  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  (1 + t\sigma_c^2) \sum_{i = 2}^{i_\ind -1}\nu_i^{2t}  +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \sum_{i \geq i_\ind}  \frac{\nu_i \lambda_2^{2t} + \sigma_c^2 (\lambda_2^{2t-2} \nu_i^{2t-2} + \lambda_2^{2t-4} \nu_i^{2t-4} + \cdots + 1)}{1 + \nsrmax^2} \\
&\leq C_\gamma \nu  \left( \frac{\lambda_2}{\lambda_1} \right)^{2t}  \max(1, t\sigma_c^2) +  C_\gamma  \left( \frac{1}{\lambda_1} \right)^{2t} \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t) 
\end{align*}
where $\nsrmax := \sigma_c/\lambda_1$ is the noise to signal ratio. And thus as long as 
\begin{align*}
t \geq \max \left\{C \frac{1}{\log(\lambda_1/\lambda_2)} \log   \left( \frac{C_{\gamma} \nu  \max(1, t \sigma_c^2)}{\epsilon}\right),  C \frac{1}{\log \lambda_1} \log   \left( \frac{C_{\gamma}  \max(\nu, n \sigma_c^2, \sigma_c^2 \nu t)}{\epsilon}\right) \right\}
\end{align*}
with probability at least $1 - \exp(- c \gamma_0^2) - \exp(-c \gamma_1^2)$, $\sin(u_t, u_1^*) \leq \epsilon$.
}
